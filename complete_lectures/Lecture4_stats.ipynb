{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# COLX 521 Lecture 4: Corpus Statistics\n",
    "\n",
    "* Advanced counting\n",
    "* Sorting\n",
    "* Simple statistics\n",
    "* Comparing corpora in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's return to counting. Yet another solution to the initialization problem for counting (and other situations) is the [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict). When you initialize a defaultdict for integers, the default value is zero, so no need to do anything but add. Defaultdicts also work for lists, sets, and dictionaries (defaulting to empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62713\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# my code here\n",
    "counts = defaultdict(int)\n",
    "\n",
    "for word in brown.words():\n",
    "    counts[word] += 1\n",
    "\n",
    "print(counts[\"the\"])\n",
    "# my code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pressure-measuring', 'rookie-of-the-year', 'anti-authoritarian', 'head-and-shoulders', 'materials-handling', 'Characteristically', 'non-discrimination', 'fifteen-sixteenths', 'policeman-murderer', 'interconnectedness', 'value-orientations', 'university-trained', 'thirteenth-century', 'fiber-photocathode', \"Journal-Bulletin's\", 'radiation-produced', 'on-again-off-again', 'Brumidi-Costaggini', 'microcytochemistry', 'handyman-carpenter', 'commander-in-chief', 'Communist-inspired', 'concentration-camp', 'student-physicists', 'abrasion-resistant', 'Kohnstamm-negative', 'twenty-five-dollar', 'disenfranchisement', 'counter-escalation', 'disproportionately', 'Nineteenth-century', 'upper-middle-class', 'finite-dimensional', 'Commander-in-Chief', 'Diethylstilbestrol', 'macro-instructions', 'lower-middle-class', 'ring-around-a-rosy', 'triphosphopyridine', 'Mainliner-Highland', 'entropy-increasing', 'Clinico-pathologic', 'Icelandic-speaking', 'ultracentrifugally', 'non-propagandistic', 'Kristallstrukturen', 'home-for-the-night', 'three-hundred-foot', 'basketball-playing', 'quasi-governmental', 'Kohnstamm-positive', 'self-consciousness', 'Radiosterilization', 'seventeen-year-old', 'quarter-to-quarter', 'Hawaiian-Americans', 'triphenylphosphine', 'more-than-ordinary', 'whichever-the-hell', 'self-righteousness', 'matter-of-factness', 'one-thousand-zloty', 'spectrophotometric', 'nineteenth-century', 'Farmer-in-the-Dell', 'misrepresentations', 'salt-fractionation', 'antifundamentalist', 'isocyanate-labeled', 'Autosuggestibility', 'productivity-share', 'founder-originator', 'self-determination', 'Jack-of-all-trades', 'junior-year-abroad', 'once-in-a-lifetime', 'morning-frightened', 'management-trained', 'oversimplification', 'trans-illumination', 'self-extinguishing', 'origin/destination', 'diethylstilbestrol', 'height-to-diameter', 'sentence-structure', 'gambler-politician', 'Florican-Inverness', 'state-administered', 'guaranteed-neutral', 'non-representation', 'phosphate-buffered', 'listener-supported', 'Congregationalists', 'country-squirehood', 'coconut-containing', 'head-in-the-clouds', 'Transcendentalists', 'cathodoluminescent', 'eighteenth-century', 'Istiqlal-sponsored', 'Eighteenth-century', 'coccidioidomycosis', 'Theatre-by-the-Sea', 'arteriolosclerosis', 'radiosterilization', 'Lieutenant-Colonel', 'near-Balkanization', 'apartment-building', 'self-dramatization', 'interrelationships', 'Poynting-Robertson', 'Inquisitor-General', 'light-transmitting', 'Galophone-Kimberly', 'stress-temperature', 'phosphorus-bridged', 'characteristically', 'electrocardiograph', 'Government-blessed', \"guerrilla-th'-wisp\"}\n"
     ]
    }
   ],
   "source": [
    "words_by_length = defaultdict(set)\n",
    "\n",
    "# my code here\n",
    "for word in brown.words():\n",
    "    words_by_length[len(word)].add(word)\n",
    "\n",
    "print(words_by_length[18])\n",
    "# my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often you need to normalize the counts in a dictionary, to control for the effect of corpus size and/or create a probability distribution. You can keep a running total or, more conveniently, just [sum](https://docs.python.org/3/library/functions.html#sum) the values of your count dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05400743374050114\n",
      "0.999999999999049\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(counts.values())\n",
    "\n",
    "for word in counts:\n",
    "    counts[word] /= total_tokens\n",
    "    \n",
    "print(counts[\"the\"])\n",
    "print(sum(counts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For easy interpretiblity, one often multiples these normalized word probabilities by some large number like 1000, at which point the resulting number can be understood as X occurences per 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.00743374050114\n"
     ]
    }
   ],
   "source": [
    "for word in counts:\n",
    "    counts[word] *= 1000\n",
    "    \n",
    "print(counts[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another common use case involving counts is removing words with high or low counts, which are often uninteresting or statistically unreliable. This is tricky, because in Python you can't delete from something you're iterating over! Unless you're very worried about lack of memory, usually easier to just create a new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56057\n",
      "13150\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(brown.words())\n",
    "\n",
    "#my code here\n",
    "new_counts = {}\n",
    "for word, count in counts.items():\n",
    "    if 5 < count < 10000:\n",
    "        new_counts[word] = count\n",
    "#my code here\n",
    "        \n",
    "counts = new_counts\n",
    "print(len(counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you're just interested in the highest (or lowest) count item, it is easy enough just to iterate over the dictionary once and remember the top scoring item. Beyond that, you'll want to do some sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was\n",
      "9777\n"
     ]
    }
   ],
   "source": [
    "highest_count_word = None\n",
    "highest_count = 0\n",
    "for word, count in counts.items():\n",
    "    if count > highest_count:\n",
    "        highest_count_word = word\n",
    "        highest_count = count\n",
    "print(highest_count_word)\n",
    "print(highest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if you're using a Counter object, the [most_common](https://docs.python.org/3/library/collections.html#collections.Counter.most_common) method is often handy. Counters have a few other neat options, for instance they can be added and subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_counts = Counter(brown.words())\n",
    "#my code here\n",
    "brown_counts.most_common(10)\n",
    "#my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 66758), (',', 63219), ('.', 53174), ('of', 38399), ('and', 29426), ('to', 27896), ('a', 23759), ('in', 21108), ('that', 11044), ('is', 10682)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank_counts = Counter(treebank.words())\n",
    "#my code here\n",
    "both_counts = brown_counts + treebank_counts\n",
    "print(both_counts.most_common(10))\n",
    "#my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple sorting of a list of objects in Python is fairly straightward. Use the [sort](https://docs.python.org/3/library/stdtypes.html#list.sort) method to sort in place, or [sorted](https://docs.python.org/3/library/functions.html#sorted) to create a new sorted list. Result is order from smallest to largest, use reverse keyword to reverse the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "nums = [3, 6, -4, 23, 0.5, 202, -24592, 3482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-24592, -4, 0.5, 3, 6, 23, 202, 3482]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, -4, 23, 0.5, 202, -24592, 3482]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3482, 202, 23, 6, 3, 0.5, -4, -24592]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that strings are generally sorted alphabetically, but once you get outside of a-z things can get unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['110', '12', '2', 'Aardvark', 'Zebra', 'aardvark', 'zebra']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\"aardvark\", \"Aardvark\", \"Zebra\", \"zebra\", \"12\", \"110\", \"2\"]\n",
    "\n",
    "# my code here\n",
    "sorted(strings)\n",
    "# my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often, though, you have a statistic associated with a group of objects (words, documents, corpora, etc.) and want to sort the objects based on the statistic. One easy strategy is to create a list of tuples where the statistic is the first element of the tuple, since sort operates on the first element of each tuple first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(69, 'fox'), (327, 'quick'), (539, 'brown'), (87925, 'the')]\n"
     ]
    }
   ],
   "source": [
    "counts = {\"the\":87925, \"quick\":327, \"brown\":539, \"fox\":69}\n",
    "\n",
    "# my code here\n",
    "sort_me = []\n",
    "for word, count in counts.items():\n",
    "    sort_me.append((count, word))\n",
    "sort_me.sort()\n",
    "print(sort_me)\n",
    "# my code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do this compactly, we should use a handy piece of Python syntax, the list comprehension, which allows you to build a new list based on an exisiting iterable in a single expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69, 'fox'), (327, 'quick'), (539, 'brown'), (87925, 'the')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(count,word) for word, count in counts.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An similarly compact way is to use the \"key\" keyword for *sort/sorting* function which allows you to specify a function which will define the value to sort a given iterable. The typical way to specify the function for this is to use a [lambda expression](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions). One advantage of this is that you just get the sorted list without having to deal with extracting what you need from tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox', 'quick', 'brown', 'the']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(counts.keys(), key=lambda x: counts[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that both list comprehensions and lambda expressions are relatively advanced Python syntax. If you are new to Python and find them confusing, it is totally okay not to use them in this course!\n",
    "\n",
    "Once you have a sorted list, you can use slicing to get what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[')', 'been', 'their', 'him', 'would', 'all', 'you', 'they', 'one', 'her', 'He', 'but', 'were', '--', 'which', 'an', 'have', 'this', 'or', 'from', 'are', 'not', '?', 'at', 'had', 'by', 'I', ';', 'be', 'on', 'his', 'he', 'as', 'it', 'with', 'The', \"''\", '``', 'for', 'was', 'is', 'that', 'in', 'a', 'to', 'and', 'of', '.', ',', 'the']\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(brown.words())\n",
    "\n",
    "sorted_words = sorted(counts.keys(), key=lambda x: counts[x])\n",
    "print(sorted_words[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Use sorting to get the 50 longest and shortest word types in the Penn Treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', ':', ',', 'I', '5', '$', '3', 'B', 'G', 'X', '7', '6', 'A', '@', '4', '0', 'a', '`', '-', '1', '&', \"'\", '%', '.', 'R', 'F', '#', '!', '9', '*', '8', '2', 'b', ';', 'Mo', 'Al', '11', '95', '75', 'GM', '94', 'wo', 'AG', '25', 'H.', '70', '68', 'We', '71', 'at']\n",
      "['Lafite-Rothschild', 'substance-abusing', 'larger-than-normal', 'industry-supported', 'investor-relations', 'telecommunications', 'Corton-Charlemagne', 'school-improvement', 'constitutional-law', 'dollar-denominated', 'computer-generated', 'stock-manipulation', 'recession-inspired', 'shareholder-rights', 'search-and-seizure', 'acquisition-minded', 'newspaper-printing', 'diethylstilbestrol', 'yttrium-containing', 'property\\\\/casualty', 'financial-services', 'Metallgesellschaft', 'housing-assistance', 'machine-gun-toting', 'Property\\\\/casualty', 'Philadelphia-based', 'automotive-lighting', 'identity-management', 'multibillion-dollar', 'limited-partnership', '238,000-circulation', 'disaster-assistance', 'less-than-brilliant', 'information-services', 'government-certified', 'language-housekeeper', 'counterrevolutionary', 'sometimes-exhausting', 'anti-morning-sickness', 'weapons-modernization', 'collective-bargaining', 'intellectual-property', 'most-likely-successor', 'Trockenbeerenauslesen', 'telephone-information', 'industrial-production', 'computer-system-design', 'Bridgestone\\\\/Firestone', 'Macmillan\\\\/McGraw-Hill', 'marketing-communications']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_length = sorted(set(treebank.words()), key=lambda x: len(x))\n",
    "print(sorted_by_length[:50])\n",
    "print(sorted_by_length[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are two other built in functions, [min](https://docs.python.org/3/library/functions.html#min) and [max](https://docs.python.org/3/library/functions.html#min) which get the minimum and maximum values. Like sort/sorted, they have a *key* keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marketing-communications\n"
     ]
    }
   ],
   "source": [
    "print(max(set(treebank.words()), key=lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are more ways to sort when you are using numpy arrays, but that is beyond our scope here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The easiest sorts of corpus statistics to calculate are averages: e.g. average word length, average sentence length, average words per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "\n",
    "def get_simple_stats(corpus):\n",
    "    num_chars = sum([len(word) for word in corpus.words()])\n",
    "    num_words = len(corpus.words())\n",
    "    num_sents = len(corpus.sents())\n",
    "    num_texts = len(corpus.fileids())\n",
    "    print(\"average word length\")\n",
    "    print(num_chars/num_words)\n",
    "    print(\"average sentence length\")\n",
    "    print(num_words/num_sents)\n",
    "    print(\"average text length\")\n",
    "    print(num_words/num_texts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average word length\n",
      "4.276538246904905\n",
      "average sentence length\n",
      "20.250994070456922\n",
      "average text length\n",
      "2322.384\n"
     ]
    }
   ],
   "source": [
    "get_simple_stats(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One popular statistic for individual texts that reflects lexical diversity is the type-token ratio. Note that when you are using it for comparison, you generally need to fix the number of tokens. As we've seen already seen, sets are an easy way to get the number of types, though you'll want to lower case first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048275392872152066\n"
     ]
    }
   ],
   "source": [
    "types = set(brown.words())\n",
    "print(len(types)/len(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.269"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = set(brown.words()[:10000])\n",
    "len(types)/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The relative quantity of the main closed-class POS can reflect the nature of a particular corpus. For POS tagged texts, this is easy to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23521002555994186\n"
     ]
    }
   ],
   "source": [
    "noun_count = 0\n",
    "for word, pos in brown.tagged_words():\n",
    "    if pos[0] == \"N\":\n",
    "        noun_count += 1\n",
    "print(noun_count/len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One popular POS summary statistic is lexical density, which can also be calculated using a POS-tagged corpus. It is the ratio of open-class words (nouns, verbs, adjectives, adverbs) to all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43480664696277616\n"
     ]
    }
   ],
   "source": [
    "open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "open_class_total = 0\n",
    "for word, pos in brown.tagged_words():\n",
    "    # my code here\n",
    "    if pos[0] in open_class_prefix:\n",
    "        open_class_total += 1\n",
    "    # my code here\n",
    "print(open_class_total/len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course, any word or POS sequence or otherwise easy identified linguistic property may be considered a potential statistic. For example, let's count how often split English infinitives (i.e. TO + RB +  V, \"to boldly go\") appear per 1000 words in the Brown (and print them out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to formally request\n",
      "to completely bypass\n",
      "to merely go\n",
      "to properly express\n",
      "to properly display\n",
      "to even name\n",
      "to magically influence\n",
      "to actually move\n",
      "to roughly calculate\n",
      "to first drill\n",
      "to first confront\n",
      "to ever leave\n",
      "to substantially lessen\n",
      "to so notify\n",
      "to fully serve\n",
      "to promptly salvage\n",
      "to virtually destroy\n",
      "to approximately quadruple\n",
      "to deliberately behave\n",
      "to often seclude\n",
      "to properly relate\n",
      "to partially destroy\n",
      "to accurately measure\n",
      "to automatically hold\n",
      "to deliberately foul\n",
      "to gradually reach\n",
      "to just throw\n",
      "0.023251968666680445\n"
     ]
    }
   ],
   "source": [
    "split_infinitives = 0\n",
    "for sent in brown.tagged_sents():\n",
    "    #my code here\n",
    "    for i in range(len(sent) - 2):\n",
    "        if sent[i][1] == \"TO\" and sent[i+1][1] == \"RB\" and sent[i+2][1][0] == \"V\":\n",
    "            print(sent[i][0], sent[i+1][0], sent[i+2][0])\n",
    "            split_infinitives += 1\n",
    "    #my code here\n",
    "        \n",
    "print(1000*split_infinitives/len(brown.words()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's pick two corpora from NLTK (your choice, but not the Brown) and do some comparisons. First, let's look (again) at vocabulary overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, switchboard\n",
    "\n",
    "guten_types = set(gutenberg.words())\n",
    "switch_types = set(switchboard.words())\n",
    "both_types = guten_types&switch_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51156\n"
     ]
    }
   ],
   "source": [
    "print(len(guten_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4729\n"
     ]
    }
   ],
   "source": [
    "print(len(switch_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3631\n"
     ]
    }
   ],
   "source": [
    "print(len(both_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vocabulary overlap will overestimate the actual difference between two small corpora (Why?). A better measure: what percentage of the tokens of each corpus consist of types that appear in both?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "def percent_in_set(word_set, corpus):\n",
    "    return len([word for word in corpus.words() if word in word_set])/len(corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.736242534653284"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_in_set(both_types,gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001473572325829"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_in_set(both_types,switchboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Now, let's look at some of the \"simple\" statistics for each corpus and see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_simple_stats_spoken(corpus):\n",
    "    num_chars = sum([len(word) for word in corpus.words()])\n",
    "    num_words = len(corpus.words())\n",
    "    num_sents = len(corpus.turns())\n",
    "    num_texts = len(corpus.fileids())\n",
    "    print(\"average word length\")\n",
    "    print(num_chars/num_words)\n",
    "    print(\"average turn length\")\n",
    "    print(num_words/num_sents)\n",
    "    print(\"average text length\")\n",
    "    print(num_words/num_texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average word length\n",
      "3.618868231123358\n",
      "average sentence length\n",
      "26.601317071190845\n",
      "average text length\n",
      "145645.16666666666\n"
     ]
    }
   ],
   "source": [
    "get_simple_stats(gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average word length\n",
      "3.240325152188617\n",
      "average turn length\n",
      "15.612294927399585\n",
      "average text length\n",
      "82792.0\n"
     ]
    }
   ],
   "source": [
    "get_simple_stats_spoken(switchboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Let's write a function that calculates the type-token ratio by using the first n words from each of the corpora. Then compare our two corpora with n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def type_token_first_n(corpus, n):\n",
    "    types = set(corpus.words()[:n])\n",
    "    return len(types)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_first_n(gutenberg,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.278"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_first_n(switchboard,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's build a dictionary of counts for both corpora and (simultaneously) calculate the percentage of tokens which are *hapax legonema* (word types that only appear once) for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "def get_counts(corpus):\n",
    "    return Counter([word.lower() for word in corpus.words()])\n",
    "\n",
    "def percent_hapax(corpus):\n",
    "    counts = get_counts(corpus)\n",
    "    hapax = [word for word in counts if counts[word] == 1]\n",
    "    print(len(hapax)/len(corpus.words()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005886452348229887\n"
     ]
    }
   ],
   "source": [
    "percent_hapax(gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02512320030920862\n"
     ]
    }
   ],
   "source": [
    "percent_hapax(switchboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's create a separate dictionary which consists of the ratios of counts of words appearing in both corpora first normalized by the size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "def normalize(counts):\n",
    "    total = sum(counts.values())\n",
    "    for word in counts:\n",
    "        counts[word] /= total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guten_counts = get_counts(gutenberg)\n",
    "normalize(guten_counts)\n",
    "switch_counts = get_counts(switchboard)\n",
    "normalize(switch_counts)\n",
    "\n",
    "ratio_dict = {}\n",
    "for word in guten_counts:\n",
    "    if word in switch_counts:\n",
    "        ratio_dict[word] = guten_counts[word]/switch_counts[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at the ratios for stopwords and punctuation and see if there are any clear differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2379976700132294"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_dict[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6690428768208987"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_dict[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694629379261056"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_dict[\",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555883871404298"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_dict[\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.25371517458908"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ratio_dict[\"however\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, let's sort the words by their ratio and look at those words with the highest and lowest ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sorted_counts = sorted([(count,word) for word,count in ratio_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0006445011727746333, 'ca'), (0.0015790278732978516, 'guy'), (0.0015790278732978516, 'um'), (0.0017544754147753906, 'dad'), (0.0019737848416223145, 'anymore'), (0.0021053704977304685, 'program'), (0.003508950829550781, 'pro'), (0.003947569683244628, 'taxes'), (0.003947569683244628, 'u'), (0.003947569683244629, 'restaurants')]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40.26521076909521, 'face'), (44.37068323966962, 'among'), (45.09703606138664, 'eyes'), (45.23914856998344, '!'), (51.41314755457804, 'therefore'), (81.85680495176061, 'israel'), (144.86001709634488, 'upon'), (284.54082276827285, 'unto'), (368.92407231731, 'shall'), (627.600418520964, \"'\")]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_counts[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Compare two English corpora in NLTK that have POS tag annotations for their lexical density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37422697845202435\n",
      "0.43480664696277616\n"
     ]
    }
   ],
   "source": [
    "def lexical_density(corpus):\n",
    "    open_class_total = 0\n",
    "    total = 0\n",
    "    for word,pos in corpus.tagged_words():\n",
    "        if pos[0] in open_class_prefix:\n",
    "            open_class_total += 1\n",
    "        total += 1\n",
    "    print(open_class_total/total)\n",
    "    \n",
    "lexical_density(switchboard)\n",
    "lexical_density(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Advanced exercise: pick a non-English corpus with POS tagging and see how its lexical density compares with the Brown (you'll have to figure out the tag scheme of this other language). Is lexical density directly comparable across languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6579610813406529\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sinica_treebank\n",
    "\n",
    "lexical_tags = {\"N\",\"V\"}\n",
    "\n",
    "lexical = 0\n",
    "total = 0\n",
    "for word,tag in sinica_treebank.tagged_words():\n",
    "    if tag[0] in lexical_tags:\n",
    "        lexical += 1\n",
    "    total += 1\n",
    "print(lexical/total)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
