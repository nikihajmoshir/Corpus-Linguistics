{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COLX 521 Lecture 7: Text Preprocessing\n",
    "\n",
    "* Sentence segmentation\n",
    "* Tokenization\n",
    "* Lemmatization and Stemming\n",
    "* POS tagging\n",
    "* End-to-end preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For many computational linguistics applications, the sentence is a key unit of processing. All modern written languages have an end-of-sentence marker. For some languages, like Chinese, this marker is unambigious and so getting the sentences of the text is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你好', '我叫志林', '我是老师', '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_text = \"你好。我叫志林。我是老师。\"\n",
    "\n",
    "# my code here\n",
    "zh_sents = zh_text.split(\"。\")\n",
    "zh_sents\n",
    "# my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, English is not so easy because the period (\".\") is ambiguous. It has various uses. This is true for many other European languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr', ' Brooke got his Ph', 'D', ' from the Univ', ' of Toronto on 12/2013', ' Dr', \" Brooke's GPA was 4\", \"1, and his thesis wasn't half-bad, if a bit too long\", ' He went into industry', '', '', 'but later came back to academia', '']\n"
     ]
    }
   ],
   "source": [
    "en_text = \"Dr. Brooke got his Ph.D. from the Univ. of Toronto on 12/2013. Dr. Brooke's GPA was 4.1, and his thesis wasn't half-bad, if a bit too long. He went into industry...but later came back to academia.\"\n",
    "\n",
    "# my code here\n",
    "print(en_text.split(\".\"))\n",
    "# my code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are ways to improve things considerably by using regular expressions; for instance, splitting only when a period is followed by a space and then an upper case letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Brooke got his Ph.D. from the Univ. of Toronto on 12/2013',\n",
       " \"Dr. Brooke's GPA was 4.1, and his thesis wasn't half-bad, if a bit too long\",\n",
       " 'He went into industry...but later came back to academia.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = r\"(?<!Dr)\\. (?=[A-Z])\"\n",
    "\n",
    "# my code here\n",
    "re.split(regex, en_text)\n",
    "# my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For major languages with significant ambiguity, you'll probably want to use a dedicated sentence splitter, which NLTK has for some languages (17). But don't expect perfection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Brooke got his Ph.D. from the Univ.',\n",
       " 'of Toronto on 12/2013.',\n",
       " \"Dr. Brooke's GPA was 4.1, and his thesis wasn't half-bad, if a bit too long.\",\n",
       " 'He went into industry...but later came back to academia.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "# my code here\n",
    "sent_tokenize(en_text)\n",
    "# my code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, breaking a sentence up into words seems like it should be easy, but actually rarely is. Spaces separate most word tokens, but punctuation is a problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dr. Brooke's GPA was 4.1, and his thesis wasn't half-bad, if a bit too long.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provided code\n",
    "sents = sent_tokenize(en_text)\n",
    "sent = sents[2]\n",
    "sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " \"Brooke's\",\n",
       " 'GPA',\n",
       " 'was',\n",
       " '4.1,',\n",
       " 'and',\n",
       " 'his',\n",
       " 'thesis',\n",
       " \"wasn't\",\n",
       " 'half-bad,',\n",
       " 'if',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'too',\n",
       " 'long.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr',\n",
       " 'Brooke',\n",
       " 's',\n",
       " 'GPA',\n",
       " 'was',\n",
       " '4',\n",
       " '1',\n",
       " 'and',\n",
       " 'his',\n",
       " 'thesis',\n",
       " 'wasn',\n",
       " 't',\n",
       " 'half',\n",
       " 'bad',\n",
       " 'if',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'too',\n",
       " 'long']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[match.group() for match in re.finditer(\"\\w+\", sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In computational linguistics for English, clitics (small words that are phonologically joined to a host word) such as \"n't\" and \"'s\" are often treated as separate words, and need to be dealt with specially). A proper word tokenizer (such as is included in NLTK) will deal with these subtle issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Brooke',\n",
       " \"'s\",\n",
       " 'GPA',\n",
       " 'was',\n",
       " '4.1',\n",
       " ',',\n",
       " 'and',\n",
       " 'his',\n",
       " 'thesis',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'half-bad',\n",
       " ',',\n",
       " 'if',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'too',\n",
       " 'long',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# my code here\n",
    "word_tokenize(sent)\n",
    "# my code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In some languages (such as Chinese) there are no spaces in the words. Much more sophisticated word segmenters are needed in this case, for example [jieba](https://github.com/fxsjy/jieba) for Chinese (you'll need to install the package to run this code). One challenge for these languages is that it isn't always clear what a word is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.860 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词 是 小菜一碟\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "from jieba import cut\n",
    "zh_text = \"分词是小菜一碟\" # tokenization is a piece of cake\n",
    "\n",
    "print(\" \".join(cut(zh_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: In English, a similar case to Chinese (and other no-space languages) occurs in the context of hashtags. Write code to automatically split the hashtags into their constituent words (you can assume only two, which makes it easier than the general case), using the NLTK words lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('follow', 'me')\n",
      "('good', 'morning')\n",
      "('happy', 'hour')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "en_words = set(words.words(\"en\"))\n",
    "\n",
    "def dehash(hashtag):\n",
    "    hashtag = hashtag.strip(\"#\")\n",
    "    # your code here\n",
    "    for i in range(1, len(hashtag) -1):\n",
    "        if hashtag[:i] in en_words and hashtag[i:] in en_words:\n",
    "            return (hashtag[:i], hashtag[i:])\n",
    "    # your code here\n",
    "\n",
    "hashtags = [\"#followme\", \"#goodmorning\",\"#happyhour\"]\n",
    "\n",
    "for hashtag in hashtags:\n",
    "    print(dehash(hashtag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some applications, it can be useful to ignore the morphological differences between words. A classic example is information retrival (i.e. web search); if a user looks for \"sleeping kitties\", you might want to include in your results a page which mentions that \"the kitty sleeps\". By default, though, \"kitty\" and \"kitties\" and \"sleeps\" and \"sleeping\" are completely different word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "text = [\"The\", \"kitty\", \"sleeps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"sleeping\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"kitties\" in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lemmatization converts an inflected form to a base, uninflected form. NLTK has a lemmatizer for English that uses the WordNet lexicon. One tricky aspect is that it requires a part-of-speech. It works for both regular and irregular forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"kitties\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleeping\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleep\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, \"n\")\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, \"v\")\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"does\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stemming has a similar purpose but it strips off both inflectional and derivational mophology to reach a stem. This stem is not often itself a word. Sometime stemming incorrectly collapses/conflates words with very different meaning. The most popular stemming algorithm for English is called the [porter stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#provided code\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "S1 = \"automatization\"\n",
    "S2 = \"automatic\"\n",
    "S3 = \"has\"\n",
    "S4 = \"have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ha'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Try both lemmatizing and stemming the sentence below (after tokenizing), and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whereas', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbarous', 'act', 'which', 'have', 'outrage', 'the', 'conscience', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'being', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'be', 'proclaim', 'a', 'the', 'highest', 'aspiration', 'of', 'the', 'common', 'people']\n",
      "['wherea', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbar', 'act', 'which', 'have', 'outrag', 'the', 'conscienc', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'be', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'been', 'proclaim', 'as', 'the', 'highest', 'aspir', 'of', 'the', 'common', 'peopl']\n"
     ]
    }
   ],
   "source": [
    "S = \"Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people\"\n",
    "words = word_tokenize(S)\n",
    "#your code here\n",
    "lemmas = [lemmatize(word) for word in words]\n",
    "print(lemmas)\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've already seen that POS tagging can be useful for doing analysis of corpora. It has other uses, for instance it can be used to focus on particular kinds of words for certain applications, and it can provide simple word sense disambiguation (e.g. the word \"cross\"). The NLTK POS tagger for English is easy to use and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('think', 'VBP'), ('the', 'DT'), ('NLTK', 'NNP'), ('pos', 'NN'), ('tagger', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('pretty', 'RB'), ('solid', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('doing', 'VBG'), ('computational', 'JJ'), ('linguistics', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "S = \"I think the NLTK pos tagger is a pretty solid tool for doing computational linguistics\"\n",
    "#my code here\n",
    "print(pos_tag(S.split(\" \")))\n",
    "#my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The standard tagset for English and used for the NLTK tagger (and most others) is the one created as part of the Penn Treebank annotation project, defined [here](https://www.anc.org/penn.html). There is another popular tagset is known as the universal POS set which is applicable to any language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " ('think', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('NLTK', 'NOUN'),\n",
       " ('pos', 'NOUN'),\n",
       " ('tagger', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('pretty', 'ADV'),\n",
       " ('solid', 'ADJ'),\n",
       " ('tool', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('doing', 'VERB'),\n",
       " ('computational', 'ADJ'),\n",
       " ('linguistics', 'NOUN')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(S.split(\" \"), tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also use NLTK to build a POS tagger for any language where we have a manually tagged corpus and/or morphological information which can be expressed in the form of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n",
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "from nltk import RegexpTagger, UnigramTagger\n",
    "from nltk.corpus import treebank \n",
    "\n",
    "patterns = [(r\".*ing$\", \"VBG\"), (r\".*ed$\", \"VBD\"),(r\".*s$\", \"NNS\"),(r\".*\", \"NN\")]\n",
    "sentence = [\"he\", \"googled\", \"cats\"]\n",
    "tagged = treebank.tagged_sents()\n",
    "\n",
    "re_tagger= RegexpTagger(patterns)\n",
    "uni_tagger= UnigramTagger(tagged, backoff=re_tagger)\n",
    "print(pos_tag(sentence))\n",
    "print(uni_tagger.tag(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Use NLTK to tokenize the sentences about me, and then, for each sentence, compare the tags from the main NLTK POS tagger and the simple one we just built. Are there many differences? When there are, which do you think is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NNP'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NN'), ('.', '.')]\n",
      "[('of', 'IN'), ('Toronto', 'NNP'), ('on', 'IN'), ('12/2013', 'CD'), ('.', '.')]\n",
      "[('of', 'IN'), ('Toronto', 'NNP'), ('on', 'IN'), ('12/2013', 'NN'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), (\"'s\", 'POS'), ('GPA', 'NNP'), ('was', 'VBD'), ('4.1', 'CD'), (',', ','), ('and', 'CC'), ('his', 'PRP$'), ('thesis', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('half-bad', 'JJ'), (',', ','), ('if', 'IN'), ('a', 'DT'), ('bit', 'NN'), ('too', 'RB'), ('long', 'RB'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), (\"'s\", 'POS'), ('GPA', 'NN'), ('was', 'VBD'), ('4.1', 'CD'), (',', ','), ('and', 'CC'), ('his', 'PRP$'), ('thesis', 'NNS'), ('was', 'VBD'), (\"n't\", 'RB'), ('half-bad', 'NN'), (',', ','), ('if', 'IN'), ('a', 'DT'), ('bit', 'NN'), ('too', 'RB'), ('long', 'JJ'), ('.', '.')]\n",
      "[('He', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('industry', 'NN'), ('...', ':'), ('but', 'CC'), ('later', 'RB'), ('came', 'VBD'), ('back', 'RB'), ('to', 'TO'), ('academia', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('industry', 'NN'), ('...', ':'), ('but', 'CC'), ('later', 'JJ'), ('came', 'VBD'), ('back', 'RB'), ('to', 'TO'), ('academia', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    # your code here\n",
    "    tokens = word_tokenize(sent)\n",
    "    print(pos_tag(tokens))\n",
    "    print(uni_tagger.tag(tokens))\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-in-one preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK isn't the only option for preprocessing in English with Python. Another popular choice is [SpaCy](https://spacy.io), which will do everything you might need in one line of code. (you'll need to install the package and its models). It's fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "#pip install spacy\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# provided code\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Hi there. How are you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can access what you need in the resulting document object. The sentences are in sents, each sentence is a list of tokens, and each token has a lemmas_, pos_ (the universal POS tag), and tag_ attribute (the treebank pos tag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there.\n",
      "Hi\n",
      "hi\n",
      "INTJ\n",
      "UH\n",
      "there\n",
      "there\n",
      "ADV\n",
      "RB\n",
      ".\n",
      ".\n",
      "PUNCT\n",
      ".\n",
      "How are you?\n",
      "How\n",
      "how\n",
      "ADV\n",
      "WRB\n",
      "are\n",
      "be\n",
      "VERB\n",
      "VBP\n",
      "you\n",
      "-PRON-\n",
      "PRON\n",
      "PRP\n",
      "?\n",
      "?\n",
      "PUNCT\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#provided code\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(token)\n",
    "        print(token.lemma_)\n",
    "        print(token.pos_)\n",
    "        print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Play around with SpaCy and NLTK for preprocessing English until you find some difference in the results of their preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NNP'), ('.', '.')]\n",
      "[('of', 'IN'), ('Toronto', 'NNP'), ('in', 'IN'), ('12/2013', 'CD'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), (\"'s\", 'POS'), ('GPA', 'NNP'), ('was', 'VBD'), ('4.3', 'CD'), (',', ','), ('and', 'CC'), ('his', 'PRP$'), ('thesis', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('half', 'RB'), ('-', 'HYPH'), ('bad', 'JJ'), (',', ','), ('if', 'IN'), ('a', 'DT'), ('bit', 'NN'), ('too', 'RB'), ('long', 'RB'), ('.', '.')]\n",
      "[('He', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('industry', 'NN'), ('...', ':'), ('but', 'CC'), ('later', 'RB'), ('came', 'VBD'), ('back', 'RB'), ('to', 'IN'), ('academia', 'NN'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NNP'), ('.', '.')]\n",
      "[('of', 'IN'), ('Toronto', 'NNP'), ('in', 'IN'), ('12/2013', 'CD'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Brooke', 'NNP'), (\"'s\", 'POS'), ('GPA', 'NNP'), ('was', 'VBD'), ('4.3', 'CD'), (',', ','), ('and', 'CC'), ('his', 'PRP$'), ('thesis', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('half-bad', 'JJ'), (',', ','), ('if', 'IN'), ('a', 'DT'), ('bit', 'NN'), ('too', 'RB'), ('long', 'RB'), ('.', '.')]\n",
      "[('He', 'PRP'), ('went', 'VBD'), ('into', 'IN'), ('industry', 'NN'), ('...', ':'), ('but', 'CC'), ('later', 'RB'), ('came', 'VBD'), ('back', 'RB'), ('to', 'TO'), ('academia', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "en_doc = nlp(en_text)\n",
    "for sent in en_doc.sents:\n",
    "    print([(str(token), token.tag_) for token in sent])\n",
    "for sent in sent_tokenize(en_text):\n",
    "    print(pos_tag(word_tokenize(sent)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exercise: Try SpaCy's [multilingual support](https://spacy.io/usage/models#languages). If you can, pick a language that you know well enough that you can tell whether it is doing a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\n",
      "Considérant\n",
      "considérer\n",
      "VERB\n",
      "VERB__Tense=Pres|VerbForm=Part\n",
      "que\n",
      "que\n",
      "SCONJ\n",
      "SCONJ___\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "méconnaissance\n",
      "méconnaissance\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "et\n",
      "et\n",
      "CCONJ\n",
      "CCONJ___\n",
      "le\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
      "mépris\n",
      "mépris\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n",
      "des\n",
      "un\n",
      "DET\n",
      "DET__Definite=Ind|Number=Plur|PronType=Art\n",
      "droits\n",
      "droit\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "l'\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Sing|PronType=Art\n",
      "homme\n",
      "homme\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n",
      "ont\n",
      "avoir\n",
      "AUX\n",
      "AUX__Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\n",
      "conduit\n",
      "conduire\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\n",
      "à\n",
      "à\n",
      "ADP\n",
      "ADP___\n",
      "des\n",
      "un\n",
      "DET\n",
      "DET__Definite=Ind|Number=Plur|PronType=Art\n",
      "actes\n",
      "acte\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Plur\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "barbarie\n",
      "barbarie\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "qui\n",
      "qui\n",
      "PRON\n",
      "PRON__PronType=Rel\n",
      "révoltent\n",
      "révolter\n",
      "VERB\n",
      "VERB__Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "conscience\n",
      "conscience\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "l'\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Sing|PronType=Art\n",
      "humanité\n",
      "humanité\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "et\n",
      "et\n",
      "CCONJ\n",
      "CCONJ___\n",
      "que\n",
      "que\n",
      "SCONJ\n",
      "SCONJ___\n",
      "l'\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Sing|PronType=Art\n",
      "avènement\n",
      "avènement\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n",
      "d'\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "un\n",
      "un\n",
      "DET\n",
      "DET__Definite=Ind|Gender=Masc|Number=Sing|PronType=Art\n",
      "monde\n",
      "monde\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n",
      "où\n",
      "où\n",
      "PRON\n",
      "PRON__PronType=Rel\n",
      "les\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Plur|PronType=Art\n",
      "êtres\n",
      "être\n",
      "NOUN\n",
      "NOUN__Number=Plur\n",
      "humains\n",
      "humain\n",
      "ADJ\n",
      "ADJ__Gender=Masc|Number=Plur\n",
      "seront\n",
      "être\n",
      "AUX\n",
      "AUX__Mood=Ind|Number=Plur|Person=3|Tense=Fut|VerbForm=Fin\n",
      "libres\n",
      "libre\n",
      "ADJ\n",
      "ADJ__Number=Plur\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "parler\n",
      "parler\n",
      "VERB\n",
      "VERB__VerbForm=Inf\n",
      "et\n",
      "et\n",
      "CCONJ\n",
      "CCONJ___\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "croire\n",
      "croire\n",
      "VERB\n",
      "VERB__VerbForm=Inf\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "libérés\n",
      "libérer\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Plur|Tense=Past|VerbForm=Part\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "terreur\n",
      "terreur\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "et\n",
      "et\n",
      "CCONJ\n",
      "CCONJ___\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "misère\n",
      "misère\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      ",\n",
      ",\n",
      "PUNCT\n",
      "PUNCT___\n",
      "a\n",
      "avoir\n",
      "AUX\n",
      "AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "été\n",
      "être\n",
      "AUX\n",
      "AUX__Tense=Past|VerbForm=Part\n",
      "proclamé\n",
      "proclamer\n",
      "VERB\n",
      "VERB__Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part|Voice=Pass\n",
      "comme\n",
      "comme\n",
      "ADP\n",
      "ADP___\n",
      "la\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
      "plus\n",
      "plus\n",
      "ADV\n",
      "ADV___\n",
      "haute\n",
      "haut\n",
      "ADJ\n",
      "ADJ__Gender=Fem|Number=Sing\n",
      "aspiration\n",
      "aspiration\n",
      "NOUN\n",
      "NOUN__Gender=Fem|Number=Sing\n",
      "de\n",
      "de\n",
      "ADP\n",
      "ADP___\n",
      "l'\n",
      "le\n",
      "DET\n",
      "DET__Definite=Def|Number=Sing|PronType=Art\n",
      "homme\n",
      "homme\n",
      "NOUN\n",
      "NOUN__Gender=Masc|Number=Sing\n"
     ]
    }
   ],
   "source": [
    "# fr_core_news_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(token)\n",
    "        print(token.lemma_)\n",
    "        print(token.pos_)\n",
    "        print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SpaCy is solid and stable, but if you want the very best, state-of-the-art NLP tools and are willing to sacrifice speed, you might also try [Flair](https://github.com/zalandoresearch/flair)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
